{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e366f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bfd9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # converts PIL Image -> Tensor, scales 0-255 to 0-1\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize RGB channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26db7a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:06<00:00, 26.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "#Lets first download/get the daat\n",
    "train_data= CIFAR10(\n",
    "    root='data',\n",
    "    download= True,\n",
    "    train= True,\n",
    "    transform= transform,\n",
    "    target_transform= None\n",
    ")\n",
    "\n",
    "test_data= CIFAR10(\n",
    "    root='data',\n",
    "    download= True,\n",
    "    transform= transform,\n",
    "    train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60e299",
   "metadata": {},
   "source": [
    "Lets see what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c941a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5373, -0.6627, -0.6078,  ...,  0.2392,  0.1922,  0.1608],\n",
       "         [-0.8745, -1.0000, -0.8588,  ..., -0.0353, -0.0667, -0.0431],\n",
       "         [-0.8039, -0.8745, -0.6157,  ..., -0.0745, -0.0588, -0.1451],\n",
       "         ...,\n",
       "         [ 0.6314,  0.5765,  0.5529,  ...,  0.2549, -0.5608, -0.5843],\n",
       "         [ 0.4118,  0.3569,  0.4588,  ...,  0.4431, -0.2392, -0.3490],\n",
       "         [ 0.3882,  0.3176,  0.4039,  ...,  0.6941,  0.1843, -0.0353]],\n",
       "\n",
       "        [[-0.5137, -0.6392, -0.6235,  ...,  0.0353, -0.0196, -0.0275],\n",
       "         [-0.8431, -1.0000, -0.9373,  ..., -0.3098, -0.3490, -0.3176],\n",
       "         [-0.8118, -0.9451, -0.7882,  ..., -0.3412, -0.3412, -0.4275],\n",
       "         ...,\n",
       "         [ 0.3333,  0.2000,  0.2627,  ...,  0.0431, -0.7569, -0.7333],\n",
       "         [ 0.0902, -0.0353,  0.1294,  ...,  0.1608, -0.5137, -0.5843],\n",
       "         [ 0.1294,  0.0118,  0.1137,  ...,  0.4431, -0.0745, -0.2784]],\n",
       "\n",
       "        [[-0.5059, -0.6471, -0.6627,  ..., -0.1529, -0.2000, -0.1922],\n",
       "         [-0.8431, -1.0000, -1.0000,  ..., -0.5686, -0.6078, -0.5529],\n",
       "         [-0.8353, -1.0000, -0.9373,  ..., -0.6078, -0.6078, -0.6706],\n",
       "         ...,\n",
       "         [-0.2471, -0.7333, -0.7961,  ..., -0.4510, -0.9451, -0.8431],\n",
       "         [-0.2471, -0.6706, -0.7647,  ..., -0.2627, -0.7333, -0.7333],\n",
       "         [-0.0902, -0.2627, -0.3176,  ...,  0.0980, -0.3412, -0.4353]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731b7f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df94bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d551e191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dfd8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE= 64\n",
    "train_data_batched= DataLoader(\n",
    "    train_data,\n",
    "    BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_data_batched= DataLoader(\n",
    "    test_data,\n",
    "    BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b849bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierCNN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, hidden_unit):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(input_shape, hidden_unit, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(hidden_unit),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_unit, hidden_unit, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(hidden_unit),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_unit, hidden_unit*2, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(hidden_unit*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_unit*2, hidden_unit*2, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(hidden_unit*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_unit*2, hidden_unit*4, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(hidden_unit*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_unit*4, hidden_unit*4, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(hidden_unit*4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.classification_layer = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_unit * 4 * 4 * 4, hidden_unit * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_unit * 4, output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.classification_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3342bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_image_classifier= ImageClassifierCNN(3,10,64).to(device)\n",
    "loss_fn= nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_image_classifier.parameters(), lr=0.001)\n",
    "EPOCHS=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7d748",
   "metadata": {},
   "source": [
    "ok now lets bring our training and testing functions, we will copy them as colab is removing their files and importing them is failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "913a64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def train_model(model:nn.Module,\n",
    "                data:torch.utils.data.DataLoader,\n",
    "                lossFn: nn.Module, \n",
    "                optimizer:torch.optim.Optimizer,\n",
    "                epochs:int,\n",
    "                accr_fun: Callable[[torch.Tensor, torch.Tensor], float],\n",
    "                device:torch.device):\n",
    "    model.to(device)\n",
    "\n",
    "    loss_per_epoch= []\n",
    "    clock_start= timer()\n",
    "    dataset_accuracy= []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_correct =0\n",
    "        total_samples= 0\n",
    "        true_accuracy= 0\n",
    "        avg_loss= 0\n",
    "        model.train()\n",
    "        for X , y in data:\n",
    "            X= X.to(device)\n",
    "            y= y.to(device)\n",
    "            logit= model(X)\n",
    "            preds= torch.argmax(logit, dim=1)\n",
    "            \n",
    "            \n",
    "            loss= lossFn(logit, y)\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total_samples += len(y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        avg_loss /= len(data)\n",
    "        loss_per_epoch.append(avg_loss)\n",
    "        true_accuracy= total_correct / total_samples\n",
    "        dataset_accuracy.append(true_accuracy)\n",
    "    \n",
    "    clock_stop= timer()\n",
    "    time_taken= clock_stop - clock_start\n",
    "        \n",
    "    return {\n",
    "        'loss_per_epoch': loss_per_epoch,\n",
    "        'time_taken' : time_taken,\n",
    "        'dataset_accuracy': dataset_accuracy\n",
    "    }\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def accuracy_function(correct:torch.Tensor, prediction: torch.Tensor)-> float:\n",
    "    rslt= (correct == prediction).sum().item()\n",
    "    size= len(correct)\n",
    "    return rslt/size\n",
    " \n",
    "def test_model(model:nn.Module,\n",
    "               data:torch.utils.data.DataLoader,\n",
    "               lossFn: nn.Module,\n",
    "               device:torch.device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_correct= 0\n",
    "    total_samples= 0\n",
    "    total_loss=0\n",
    "    with torch.inference_mode():\n",
    "        for X , y in data:\n",
    "            X= X.to(device)\n",
    "            y= y.to(device)\n",
    "            logits= model(X)\n",
    "            pred= torch.argmax(logits, dim=1)\n",
    "            total_correct += (y == pred).sum().item()\n",
    "            total_samples += len(y)\n",
    "            \n",
    "            loss= lossFn(logits, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    accuracy= total_correct / total_samples\n",
    "    loss= total_loss / len(data)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': loss\n",
    "    }\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e1bc1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a053e146f414646b2d95afad0086d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_per_epoch': [1.557239819060811, 1.1446424977248892, 0.9442921026283518, 0.816678189217587, 0.7121387140067947, 0.6222156959269053, 0.5550061783674732, 0.48704205649664334, 0.421965798282105, 0.3694604803019625, 0.3251027540801584, 0.2901939593465127, 0.24638946776461723, 0.21921199079498152, 0.19561422292305075, 0.17181865965752194, 0.1471413661561468, 0.14252033700590092, 0.12175814736791699, 0.11059564474703329], 'time_taken': 333.37006972300003, 'dataset_accuracy': [0.42384, 0.59296, 0.6702, 0.71876, 0.75686, 0.78872, 0.81482, 0.8375, 0.85782, 0.87492, 0.89022, 0.9012, 0.91526, 0.9259, 0.9331, 0.94172, 0.9497, 0.95214, 0.958, 0.96312]}\n"
     ]
    }
   ],
   "source": [
    "result= train_model(model_image_classifier, train_data_batched,loss_fn, optimizer, EPOCHS,accuracy_function,device)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8108a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8548, 'loss': 0.7073405951641167}\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(model_image_classifier, test_data_batched, loss_fn, device)\n",
    "print(test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
